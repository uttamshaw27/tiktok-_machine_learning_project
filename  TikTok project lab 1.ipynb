{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtNBZFHO3M7n"
   },
   "source": [
    "# **TikTok Project**\n",
    "**Course 6 - The nuts and bolts of machine learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgSbVJvomcVa",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**The purpose** of this model is to mitigate misinformation in videos on the TikTok platform.\n",
    "\n",
    "**The goal** of this model is to predict whether a TikTok video presents a \"claim\" or presents an \"opinion\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tCni9wAGphb0"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report, accuracy_score, precision_score, \\\n\u001b[0;32m     15\u001b[0m recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_importance\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "# Import packages for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import packages for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import packages for data preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Import packages for data modeling\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, \\\n",
    "recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "id": "P_CZ-1P1Q62x",
    "outputId": "f960e90c-ed45-41cf-bdaa-598b1e8b4fc0"
   },
   "outputs": [],
   "source": [
    "# Load dataset into dataframe\n",
    "data = pd.read_csv(\"tiktok_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "2rfk95MLp4a_",
    "outputId": "6fbea70a-94ef-4f1d-e9c4-822567aa296f"
   },
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kYwW-G1WqX3R",
    "outputId": "b777029a-24f3-4dfc-ef24-d5bf1f4b8b46"
   },
   "outputs": [],
   "source": [
    "# Get number of rows and columns\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URuRVjUZ_Axg"
   },
   "source": [
    "Get basic information about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zyyKCGzCp7SS",
    "outputId": "0505065b-e64f-4fe7-f5d0-01a224b82c3f"
   },
   "outputs": [],
   "source": [
    "# Get basic information\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0T5Ieb6WB61Q"
   },
   "source": [
    "Generate basic descriptive statistics about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "HbpuAS2UqY01",
    "outputId": "db89b407-30ba-405e-c4ce-5e3ccc405b57"
   },
   "outputs": [],
   "source": [
    "# Generate basic descriptive stats\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OpeNQDdyIT6"
   },
   "source": [
    "Check for and handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BrunHcfa7xnT",
    "outputId": "d958366a-c57a-4b41-d03b-b8922bbdd09b"
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LiadUrXRqQhg"
   },
   "source": [
    "**Exemplar response:** There are very few missing values relative to the number of samples in the dataset. Therefore, observations with missing values can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SHSj1Hma914I"
   },
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "data = data.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XcfffpANyNiu"
   },
   "source": [
    "Check for and handle duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IKaGnWIsiHpH",
    "outputId": "a4027a67-418c-47f5-8eb8-db74c51dc294"
   },
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scLLbNSIjAWs"
   },
   "source": [
    "There are no duplicate observations in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ecg5b3Jwfyxa"
   },
   "source": [
    "Check class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dQauKR11fyxb",
    "outputId": "7606a6ca-0db4-4002-ca56-84da7926a33c"
   },
   "outputs": [],
   "source": [
    "# Check class balance\n",
    "data[\"claim_status\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePYWt2p5LqlW"
   },
   "source": [
    "**Exemplar response:** Approximately 50.3% of the dataset represents claims and 49.7% represents opinions, so the outcome variable is balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "a0IaKI8dnG9h",
    "outputId": "c28076fb-a280-47e0-e925-1c90dac4a9a3"
   },
   "outputs": [],
   "source": [
    "# Create `text_length` column\n",
    "data['text_length'] = data['video_transcription_text'].str.len()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2RHONw6D3R6"
   },
   "source": [
    "Calculate the average `text_length` for claims and opinions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "nxLJEfLM5jEi",
    "outputId": "47f72289-3ee7-49b4-a4ef-9cc32ef05bf1"
   },
   "outputs": [],
   "source": [
    "data[['claim_status', 'text_length']].groupby('claim_status').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGu7ipi4AJmP"
   },
   "source": [
    "Visualize the distribution of `text_length` for claims and opinions using a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "MSq136S3TIYe",
    "outputId": "8c61a3f1-83ea-4d14-d874-a31ac2cc0dcf"
   },
   "outputs": [],
   "source": [
    "# Visualize the distribution of `video_transcription_text` length for claims and opinions\n",
    "# Create two histograms in one plot\n",
    "\n",
    "sns.histplot(data=data, stat=\"count\", multiple=\"dodge\", x=\"text_length\",\n",
    "             kde=False, palette=\"pastel\", hue=\"claim_status\",\n",
    "             element=\"bars\", legend=True)\n",
    "plt.xlabel(\"video_transcription_text length (number of characters)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of video_transcription_text length for claims and opinions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5acRzDOTB_7"
   },
   "source": [
    "Letter count distributions for both claims and opinions are approximately normal with a slight right skew. Claim videos tend to have more characters&mdash;about 13 more on average, as indicated in a previous cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VZowX9rhU1o"
   },
   "source": [
    "**Feature selection and transformation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTCrz71_UnHS"
   },
   "source": [
    "Encode target and catgorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "DHeI3AVr309a",
    "outputId": "6f6f06ad-9453-46dd-990c-ddd688e8d319"
   },
   "outputs": [],
   "source": [
    "X = data.copy()\n",
    "# Drop unnecessary columns\n",
    "X = X.drop(['#', 'video_id'], axis=1)\n",
    "# Encode target variable\n",
    "X['claim_status'] = X['claim_status'].replace({'opinion': 0, 'claim': 1})\n",
    "# Dummy encode remaining categorical values\n",
    "X = pd.get_dummies(X,\n",
    "                   columns=['verified_status', 'author_ban_status'],\n",
    "                   drop_first=True)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZR2BdQxZQjN"
   },
   "source": [
    "### **Task 4. Split the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uH6jiQECBgFn"
   },
   "outputs": [],
   "source": [
    "# Isolate target variable\n",
    "y = X['claim_status']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPxEovpUBZfk"
   },
   "source": [
    "Isolate the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "9NYDoItCJUR-",
    "outputId": "4d3ddaec-7ffc-4df1-b15b-f712ea8e3835"
   },
   "outputs": [],
   "source": [
    "# Isolate features\n",
    "X = X.drop(['claim_status'], axis=1)\n",
    "\n",
    "# Display first few rows of features dataframe\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHOuraMm1jRy"
   },
   "source": [
    "#### **Task 5: Create train/validate/test sets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tNw9_y9jmY1"
   },
   "source": [
    "Split data into training and testing sets, 80/20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jKgrew0V6o_3"
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_tr, X_test, y_tr, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQq9zKScz-NJ"
   },
   "source": [
    "Split the training set into training and validation sets, 75/25, to result in a final ratio of 60/20/20 for train/validate/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HhkqFZaJYTyK"
   },
   "outputs": [],
   "source": [
    "# Split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_tr, y_tr, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VlklzoujrAR"
   },
   "source": [
    "Confirm that the dimensions of the training, validation, and testing sets are in alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xgbB1NCtfxcc",
    "outputId": "5458052e-8cc4-4799-fa8b-c727ec5ce9a8"
   },
   "outputs": [],
   "source": [
    "# Get shape of each training, validation, and testing set\n",
    "X_train.shape, X_val.shape, X_test.shape, y_train.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "4r-9NHYkq8gT",
    "outputId": "93acd2c2-947a-48d8-b8d0-37e6484712d9"
   },
   "outputs": [],
   "source": [
    "# Set up a `CountVectorizer` object, which converts a collection of text to a matrix of token counts\n",
    "count_vec = CountVectorizer(ngram_range=(2, 3),\n",
    "                            max_features=15,\n",
    "                            stop_words='english')\n",
    "count_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVPNQyJtNJ2V"
   },
   "source": [
    "Fit the vectorizer to the training data (generate the n-grams) and transform it (tally the occurrences). Only fit to the training data, not the validation or test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_4Y5K5pbrFD4",
    "outputId": "a021dca1-a9a9-4f11-8d5d-1761c2673d52"
   },
   "outputs": [],
   "source": [
    "# Extract numerical features from `video_transcription_text` in the training set\n",
    "count_data = count_vec.fit_transform(X_train['video_transcription_text']).toarray()\n",
    "count_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "LB31J6a7tm2b",
    "outputId": "9c39b5c0-1b8c-4d4b-e25e-a6c065b3a81c"
   },
   "outputs": [],
   "source": [
    "# Place the numerical representation of `video_transcription_text` from training set into a dataframe\n",
    "count_df = pd.DataFrame(data=count_data, columns=count_vec.get_feature_names_out())\n",
    "\n",
    "# Display first few rows\n",
    "count_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "qKG1TK-KEfuB",
    "outputId": "8209e802-bd08-4be0-95a9-d21ecd23d076"
   },
   "outputs": [],
   "source": [
    "# Concatenate `X_train` and `count_df` to form the final dataframe for training data (`X_train_final`)\n",
    "# Note: Using `.reset_index(drop=True)` to reset the index in X_train after dropping `video_transcription_text`,\n",
    "# so that the indices align with those in `X_train` and `count_df`\n",
    "X_train_final = pd.concat([X_train.drop(columns=['video_transcription_text']).reset_index(drop=True), count_df], axis=1)\n",
    "\n",
    "# Display first few rows\n",
    "X_train_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3zgU6MZpyM7"
   },
   "source": [
    "Get n-gram counts for the validation data. Notice that the vectorizer is not being refit to the validation data. It's only transforming it. In other words, the transcriptions of the videos in the validation data are only being checked against the n-grams found in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dagpxNJup9fJ",
    "outputId": "73e12707-e35a-4343-aec5-4ae04c16735e"
   },
   "outputs": [],
   "source": [
    "# Extract numerical features from `video_transcription_text` in the testing set\n",
    "validation_count_data = count_vec.transform(X_val['video_transcription_text']).toarray()\n",
    "validation_count_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "N4cC6gu5qPj2",
    "outputId": "46181cc9-32a2-4410-c4c5-c4ca5f2179c9"
   },
   "outputs": [],
   "source": [
    "# Place the numerical representation of `video_transcription_text` from validation set into a dataframe\n",
    "validation_count_df = pd.DataFrame(data=validation_count_data, columns=count_vec.get_feature_names_out())\n",
    "validation_count_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "S3tk8bMKqsQu",
    "outputId": "fa1c159e-ad39-440c-e06b-62184d4e3fd1"
   },
   "outputs": [],
   "source": [
    "# Concatenate `X_val` and `validation_count_df` to form the final dataframe for training data (`X_val_final`)\n",
    "# Note: Using `.reset_index(drop=True)` to reset the index in X_val after dropping `video_transcription_text`,\n",
    "# so that the indices align with those in `validation_count_df`\n",
    "X_val_final = pd.concat([X_val.drop(columns=['video_transcription_text']).reset_index(drop=True), validation_count_df], axis=1)\n",
    "\n",
    "# Display first few rows\n",
    "X_val_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMw8Gd_iqmv_"
   },
   "source": [
    "Repeat the process to get n-gram counts for the test data. Again, don't refit the vectorizer to the test data. Just transform it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "PPDlmol1qrMS",
    "outputId": "eea139f1-658a-4639-a7a4-b0cdb8a873e6"
   },
   "outputs": [],
   "source": [
    "# Extract numerical features from `video_transcription_text` in the testing set\n",
    "test_count_data = count_vec.transform(X_test['video_transcription_text']).toarray()\n",
    "\n",
    "# Place the numerical representation of `video_transcription_text` from test set into a dataframe\n",
    "test_count_df = pd.DataFrame(data=test_count_data, columns=count_vec.get_feature_names_out())\n",
    "\n",
    "# Concatenate `X_val` and `validation_count_df` to form the final dataframe for training data (`X_val_final`)\n",
    "X_test_final = pd.concat([X_test.drop(columns=['video_transcription_text']\n",
    "                                      ).reset_index(drop=True), test_count_df], axis=1)\n",
    "X_test_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o16nL7LWQGfY"
   },
   "source": [
    "### **Task 6. Build models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_es-Jh1atUz"
   },
   "source": [
    "### **Build a random forest model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vAb45ShmuwLC"
   },
   "outputs": [],
   "source": [
    "# Instantiate the random forest classifier\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "\n",
    "# Create a dictionary of hyperparameters to tune\n",
    "cv_params = {'max_depth': [5, 7, None],\n",
    "             'max_features': [0.3, 0.6],\n",
    "            #  'max_features': 'auto'\n",
    "             'max_samples': [0.7],\n",
    "             'min_samples_leaf': [1,2],\n",
    "             'min_samples_split': [2,3],\n",
    "             'n_estimators': [75,100,200],\n",
    "             }\n",
    "\n",
    "# Define a dictionary of scoring metrics to capture\n",
    "scoring = {'accuracy', 'precision', 'recall', 'f1'}\n",
    "\n",
    "# Instantiate the GridSearchCV object\n",
    "rf_cv = GridSearchCV(rf, cv_params, scoring=scoring, cv=5, refit='recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note this cell might take several minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 152
    },
    "id": "42reuVJmu8Tt",
    "outputId": "3394a80f-e031-4887-8584-47932a46eccb"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "rf_cv.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qr5k5yvwvGPC",
    "outputId": "d015e2ea-8648-44c0-e94e-52e069b2910d"
   },
   "outputs": [],
   "source": [
    "# Examine best recall score\n",
    "rf_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qieU0tPFvLUA",
    "outputId": "f09592e3-c2dd-4f77-cfc6-ab0bcfe028f9"
   },
   "outputs": [],
   "source": [
    "# Examine best parameters\n",
    "rf_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Mf2TEWBS-hP"
   },
   "source": [
    "### **Build an XGBoost model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E0vq2MncTLUM"
   },
   "outputs": [],
   "source": [
    "# Instantiate the XGBoost classifier\n",
    "xgb = XGBClassifier(objective='binary:logistic', random_state=0)\n",
    "\n",
    "# Create a dictionary of hyperparameters to tune\n",
    "cv_params = {'max_depth': [4,8,12],\n",
    "             'min_child_weight': [3, 5],\n",
    "             'learning_rate': [0.01, 0.1],\n",
    "             'n_estimators': [300, 500]\n",
    "             }\n",
    "\n",
    "# Define a dictionary of scoring metrics to capture\n",
    "scoring = {'accuracy', 'precision', 'recall', 'f1'}\n",
    "\n",
    "# Instantiate the GridSearchCV object\n",
    "xgb_cv = GridSearchCV(xgb, cv_params, scoring=scoring, cv=5, refit='recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note this cell might take several minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aiSKpucwToRW"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "xgb_cv.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ny_s71DDTuD5"
   },
   "outputs": [],
   "source": [
    "xgb_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LZEwxPJiWWRm"
   },
   "outputs": [],
   "source": [
    "xgb_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyepBhCTa1Yx"
   },
   "source": [
    "### **Task 7. Evaluate models**\n",
    "\n",
    "Evaluate models against validation data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gM9egturW1eX"
   },
   "source": [
    "#### **Random forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZQbthy93bWM"
   },
   "outputs": [],
   "source": [
    "# Use the random forest \"best estimator\" model to get predictions on the validation set\n",
    "y_pred = rf_cv.best_estimator_.predict(X_val_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNjDzuqmYU0G"
   },
   "source": [
    "Display the predictions on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tyKjLA_gYUYZ"
   },
   "outputs": [],
   "source": [
    "# Display the predictions on the validation set\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXDp4m9dYlN3"
   },
   "source": [
    "\n",
    "Display the true labels of the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JymZrHVDYdvu"
   },
   "outputs": [],
   "source": [
    "# Display the true labels of the validation set\n",
    "y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVMG0ubSXQvS"
   },
   "source": [
    "Create a confusion matrix to visualize the results of the classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a confusion matrix to visualize the results of the classification model\n",
    "\n",
    "# Compute values for confusion matrix\n",
    "log_cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "# Create display of confusion matrix\n",
    "log_disp = ConfusionMatrixDisplay(confusion_matrix=log_cm, display_labels=None)\n",
    "\n",
    "# Plot confusion matrix\n",
    "log_disp.plot()\n",
    "\n",
    "# Display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFTNAnk9ehWp"
   },
   "source": [
    "The upper-left quadrant displays the number of true negatives: the number of opinions that the model accurately classified as so.\n",
    "\n",
    "The upper-right quadrant displays the number of false positives: the number of opinions that the model misclassified as claims.\n",
    "\n",
    "The lower-left quadrant displays the number of false negatives: the number of claims that the model misclassified as opinions.\n",
    "\n",
    "The lower-right quadrant displays the number of true positives: the number of claims that the model accurately classified as so.\n",
    "\n",
    "A perfect model would yield all true negatives and true positives, and no false negatives or false positives.\n",
    "\n",
    "As the above confusion matrix shows, this model does not produce any false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AMxCzRa-bnUe"
   },
   "outputs": [],
   "source": [
    "# Create a classification report\n",
    "# Create classification report for random forest model\n",
    "target_labels = ['opinion', 'claim']\n",
    "print(classification_report(y_val, y_pred, target_names=target_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeQzSiGaVBgd"
   },
   "source": [
    "#### **XGBoost**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lN_qN9ecdCPr"
   },
   "source": [
    "Now, evaluate the XGBoost model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8kDW1frqU1gA"
   },
   "outputs": [],
   "source": [
    "#Evaluate XGBoost model\n",
    "y_pred = xgb_cv.best_estimator_.predict(X_val_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-DOoQk8Vnk0"
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zVswk-gHXLG0"
   },
   "outputs": [],
   "source": [
    "# Compute values for confusion matrix\n",
    "log_cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "# Create display of confusion matrix\n",
    "log_disp = ConfusionMatrixDisplay(confusion_matrix=log_cm, display_labels=None)\n",
    "\n",
    "# Plot confusion matrix\n",
    "log_disp.plot()\n",
    "\n",
    "# Display plot\n",
    "plt.title('XGBoost - validation set');\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-BIEgcNxvGu-"
   },
   "outputs": [],
   "source": [
    "# Create a classification report\n",
    "target_labels = ['opinion', 'claim']\n",
    "print(classification_report(y_val, y_pred, target_names=target_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-HHbB2MvYAR"
   },
   "source": [
    "### **Use champion model to predict on test data**\n",
    "\n",
    "Both random forest and XGBoost model architectures resulted in nearly perfect models. Nonetheless, in this case random forest performed a little bit better, so it is the champion model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c3WCYnKXvjF3"
   },
   "outputs": [],
   "source": [
    "# Use champion model to predict on test data\n",
    "y_pred = rf_cv.best_estimator_.predict(X_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBxtLiVxvt9Z"
   },
   "outputs": [],
   "source": [
    "# Compute values for confusion matrix\n",
    "log_cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Create display of confusion matrix\n",
    "log_disp = ConfusionMatrixDisplay(confusion_matrix=log_cm, display_labels=None)\n",
    "\n",
    "# Plot confusion matrix\n",
    "log_disp.plot()\n",
    "\n",
    "# Display plot\n",
    "plt.title('Random forest - test set');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLktqdiEwRY2"
   },
   "source": [
    "#### **Feature importances of champion model**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C10wq1EGv0ta"
   },
   "outputs": [],
   "source": [
    "importances = rf_cv.best_estimator_.feature_importances_\n",
    "rf_importances = pd.Series(importances, index=X_test_final.columns)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rf_importances.plot.bar(ax=ax)\n",
    "ax.set_title('Feature importances')\n",
    "ax.set_ylabel('Mean decrease in impurity')\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
